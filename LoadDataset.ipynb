{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lobral2728/ucb_ml_capstone/blob/colab/LoadDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84ff90a6-4561-4538-8a36-31e02dbb8d3b",
      "metadata": {
        "id": "84ff90a6-4561-4538-8a36-31e02dbb8d3b"
      },
      "source": [
        "# Capstone Dataset Builder (Balanced Adults from FairFace + Avatars + Animal Faces)\n",
        "\n",
        "OUTPUT: data/final/{train,val,test}/{human,avatar,animal}/*.jpg (224x224 JPEG)\n",
        "TARGET: ResNet50 fine-tuning (PyTorch/TF compatible ImageFolder)\n",
        "\n",
        "------------------------------------\n",
        "REMINDER (future enhancement option):\n",
        "------------------------------------\n",
        "If needed later, we can add a face-detection step (e.g., MTCNN or RetinaFace)\n",
        "to crop tighter face regions before resizing. This is *not* enabled now.\n",
        "\n",
        "-------------------------\n",
        "DATASET SOURCES + CITED:\n",
        "-------------------------\n",
        "• FairFace (padding = 1.25 images + train/val labels)\n",
        "  Repo: https://github.com/joojs/fairface\n",
        "  Images (padding=1.25): https://drive.google.com/file/d/1g7qNOZz9wC7OfOhcPqH1EZ5bk1UFGmlL/view\n",
        "  Labels (train):        https://drive.google.com/file/d/1i1L3Yqwaio7YSOCj7ftgk8ZZchPG7dmH/view\n",
        "  Labels (val):          https://drive.google.com/file/d/1wOdja-ezstMEp81tX1a-EYkFebev4h7D/view\n",
        "  Paper (cite if you publish): Karkkainen & Joo (WACV 2021)\n",
        "    @inproceedings{karkkainenfairface,\n",
        "      title={FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation},\n",
        "      author={Karkkainen, Kimmo and Joo, Jungseock},\n",
        "      booktitle={WACV},\n",
        "      year={2021}, pages={1548--1558}\n",
        "    }\n",
        "\n",
        "• Avatars (Google’s Cartoon Set via Kaggle):\n",
        "  https://www.kaggle.com/datasets/brendanartley/cartoon-faces-googles-cartoon-set\n",
        "\n",
        "• Animal faces (Dogs vs Cats via Kaggle):\n",
        "  https://www.kaggle.com/datasets/salader/dogs-vs-cats\n",
        "\n",
        "NOTE: You need a working Kaggle API (place kaggle.json under ~/.kaggle/ with chmod 600)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fbefcdb-0c40-41d5-a2cb-1f1e1486f5e1",
      "metadata": {
        "id": "7fbefcdb-0c40-41d5-a2cb-1f1e1486f5e1"
      },
      "outputs": [],
      "source": [
        "!python3 -m pip install --upgrade pip\n",
        "!pip install pandas\n",
        "!pip install tqdm\n",
        "!pip install kaggle\n",
        "# Jupyter-friendly installs (no-op if already present)\n",
        "!pip -q install gdown imagehash kaggle --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eQLk2bBcdqZH",
      "metadata": {
        "id": "eQLk2bBcdqZH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8c6880-9dae-4a1f-b068-5125d189feb0",
      "metadata": {
        "id": "7f8c6880-9dae-4a1f-b068-5125d189feb0"
      },
      "outputs": [],
      "source": [
        "import os, io, sys, json, math, shutil, random, zipfile, tarfile, hashlib, gdown, imagehash\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm.auto import tqdm\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import userdata\n",
        "  from google.colab import drive\n",
        "  import shutil\n",
        "  from pathlib import Path\n",
        "  RUNNING_IN_COLAB = True\n",
        "else:\n",
        "  RUNNING_IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e94c60e-236b-4772-9746-173a9a064787",
      "metadata": {
        "id": "2e94c60e-236b-4772-9746-173a9a064787"
      },
      "source": [
        "## Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d3b15b0-0179-4e08-a26a-d19ade225500",
      "metadata": {
        "id": "5d3b15b0-0179-4e08-a26a-d19ade225500"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e777cfd-8168-47cf-bbc0-9173defffe7f",
      "metadata": {
        "id": "0e777cfd-8168-47cf-bbc0-9173defffe7f"
      },
      "source": [
        "## USER TUNABLES (easy knobs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0356cd4c-3fc0-4914-86be-44e62732216b",
      "metadata": {
        "id": "0356cd4c-3fc0-4914-86be-44e62732216b"
      },
      "outputs": [],
      "source": [
        "TARGET_PER_CLASS = 10_000  # (#3) change this to scale dataset size per class\n",
        "SPLIT = {\"train\": 0.80, \"val\": 0.10, \"test\": 0.10}\n",
        "IMG_SIZE = 224\n",
        "JPEG_QUALITY = 95\n",
        "\n",
        "# Switchable image-quality gates (your #3 request)\n",
        "APPLY_IMAGE_FILTERS = True          # turn filters on/off globally\n",
        "MIN_SHORT_SIDE_PX = 96              # reject if min(width, height) < this\n",
        "MIN_ASPECT_RATIO   = 0.50           # reject if (w/h) < this\n",
        "MAX_ASPECT_RATIO   = 2.00           # reject if (w/h) > this\n",
        "PRINT_REJECTION_MESSAGES = True     # log every rejected image (can be verbose)\n",
        "\n",
        "# Kaggle slugs (per your links)\n",
        "KAGGLE_AVATARS_DATASET = \"brendanartley/cartoon-faces-googles-cartoon-set\"\n",
        "KAGGLE_ANIMALFACES_DATASET = \"salader/dogs-vs-cats\"\n",
        "\n",
        "# Optional: restrict to specific subfolders inside those Kaggle zips (usually not needed)\n",
        "AVATAR_INCLUDE_DIRS = None\n",
        "ANIMAL_INCLUDE_DIRS = None\n",
        "\n",
        "# Safety checks\n",
        "MIN_REQUIRED_PER_CLASS = TARGET_PER_CLASS\n",
        "\n",
        "# Direct Google Drive file IDs for FairFace (padding=1.25) + labels (from joojs/fairface README)\n",
        "FAIRFACE_IMG_PAD125_ID   = \"1g7qNOZz9wC7OfOhcPqH1EZ5bk1UFGmlL\"\n",
        "FAIRFACE_LABEL_TRAIN_ID  = \"1i1L3Yqwaio7YSOCj7ftgk8ZZchPG7dmH\"\n",
        "FAIRFACE_LABEL_VAL_ID    = \"1wOdja-ezstMEp81tX1a-EYkFebev4h7D\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e37629dd-f33b-489e-a5b6-37874c99743e",
      "metadata": {
        "id": "e37629dd-f33b-489e-a5b6-37874c99743e"
      },
      "source": [
        "## Path management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a4f20ed-abf8-468d-8935-3a200586994b",
      "metadata": {
        "id": "6a4f20ed-abf8-468d-8935-3a200586994b"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path.cwd()\n",
        "WORK_DIR = BASE_DIR / \"data\"\n",
        "RAW_DIR  = WORK_DIR / \"raw\"\n",
        "OUT_DIR  = WORK_DIR / \"final\"\n",
        "TMP_DIR  = WORK_DIR / \"tmp\"\n",
        "FAIRFACE_DIR = RAW_DIR / \"fairface\"  # will hold pad=1.25 images & label CSVs\n",
        "for p in [RAW_DIR, OUT_DIR, TMP_DIR, FAIRFACE_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22c0a8dd-6895-41ed-8bd5-5c888f994be3",
      "metadata": {
        "id": "22c0a8dd-6895-41ed-8bd5-5c888f994be3"
      },
      "source": [
        "## Dependencies / environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cde79afe-8887-48ab-8535-1c9c3eafb284",
      "metadata": {
        "id": "cde79afe-8887-48ab-8535-1c9c3eafb284"
      },
      "outputs": [],
      "source": [
        "if RUNNING_IN_COLAB:\n",
        "  # Set Kaggle credentials as environment variables\n",
        "  os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "  os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "  # Define the paths (assuming BASE_DIR is defined elsewhere, e.g., Path.cwd())\n",
        "  # BASE_DIR = Path.cwd()\n",
        "  COLAB_DATA_DIR = BASE_DIR / \"data\"\n",
        "  DRIVE_DATA_PARENT_DIR = Path(\"/content/drive/My Drive/Colab Data\") # Change if you want a different parent dir in Drive\n",
        "  DRIVE_DATA_DIR = DRIVE_DATA_PARENT_DIR / \"Capstone\" # Change \"your_dataset_name\"\n",
        "else:\n",
        "  # Kaggle creds sanity\n",
        "  kaggle_creds = Path.home() / \".kaggle\" / \"kaggle.json\"\n",
        "  if not kaggle_creds.exists():\n",
        "      print(\"Kaggle credentials not found at ~/.kaggle/kaggle.json — Kaggle downloads will fail until you add them.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "495d4f7f-a466-4f6d-92a3-f95aa029ba82",
      "metadata": {
        "id": "495d4f7f-a466-4f6d-92a3-f95aa029ba82"
      },
      "source": [
        "## Helper utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61749f14-9103-4f43-9de0-830ea310346f",
      "metadata": {
        "id": "61749f14-9103-4f43-9de0-830ea310346f"
      },
      "source": [
        "### Idempotent download helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b36119-d2c8-40f7-b00b-570744573af5",
      "metadata": {
        "id": "35b36119-d2c8-40f7-b00b-570744573af5"
      },
      "outputs": [],
      "source": [
        "def _any_images_in_dir(root: Path) -> bool:\n",
        "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "    return any(p.suffix.lower() in exts for p in root.rglob(\"*\") if p.is_file())\n",
        "\n",
        "def _any_archives_in_dir(root: Path) -> bool:\n",
        "    return any(root.rglob(\"*.zip\")) or any(root.rglob(\"*.tar\")) or any(root.rglob(\"*.tar.gz\")) or any(root.rglob(\"*.tgz\"))\n",
        "\n",
        "def extract_all_archives(root: Path):\n",
        "    \"\"\"Extract all archives (recursively finds new ones after extraction).\"\"\"\n",
        "    did_any = True\n",
        "    extracted_archives = set()\n",
        "    while did_any:\n",
        "        did_any = False\n",
        "        archives_to_extract = []\n",
        "\n",
        "        # Only look for zip files in the root directory\n",
        "        zip_files = list(root.glob(\"*.zip\"))\n",
        "        # Only look for tar files in the root directory\n",
        "        tar_files = list(root.glob(\"*.tar\")) + list(root.glob(\"*.tar.gz\")) + list(root.glob(\"*.tgz\"))\n",
        "\n",
        "        archives_to_extract.extend(zip_files)\n",
        "        archives_to_extract.extend(tar_files)\n",
        "\n",
        "        for archive_path in archives_to_extract:\n",
        "            if archive_path not in extracted_archives:\n",
        "                try:\n",
        "                    extract_archive(archive_path, archive_path.parent)\n",
        "                    did_any = True\n",
        "                    extracted_archives.add(archive_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to extract {archive_path.name}: {e}\")\n",
        "    return\n",
        "\n",
        "\n",
        "def is_fairface_present(ff_dir: Path) -> bool:\n",
        "    # Look for train/val with images\n",
        "    train_dirs = [\n",
        "        ff_dir / \"train\",\n",
        "        ff_dir / \"val\",\n",
        "        ff_dir / \"fairface-img-margin125-trainval\" / \"train\",\n",
        "        ff_dir / \"fairface-img-margin125-trainval\" / \"val\",\n",
        "    ]\n",
        "    has_imgs = any(_any_images_in_dir(d) for d in train_dirs if d.exists())\n",
        "    labels_ok = (ff_dir / \"fairface_label_train.csv\").exists() and (ff_dir / \"fairface_label_val.csv\").exists()\n",
        "    return has_imgs and labels_ok\n",
        "\n",
        "def ensure_fairface_downloaded():\n",
        "    \"\"\"\n",
        "    Ensure FairFace (padding=1.25) images and labels exist & are extracted.\n",
        "    Skips re-download if already present; prints informative status.\n",
        "    \"\"\"\n",
        "    ff_zip = FAIRFACE_DIR / \"fairface-img-margin125-trainval.zip\"\n",
        "    lbl_train = FAIRFACE_DIR / \"fairface_label_train.csv\"\n",
        "    lbl_val   = FAIRFACE_DIR / \"fairface_label_val.csv\"\n",
        "\n",
        "    if is_fairface_present(FAIRFACE_DIR):\n",
        "        print(f\"Found existing FairFace (padding=1.25) in {FAIRFACE_DIR}; skipping download.\")\n",
        "        return\n",
        "\n",
        "    print(\"FairFace not fully present; checking files…\")\n",
        "    # Download missing pieces\n",
        "    if not ff_zip.exists():\n",
        "        print(\"Downloading FairFace (padding=1.25) image pack…\")\n",
        "        gdown.download(id=FAIRFACE_IMG_PAD125_ID, output=str(ff_zip), quiet=False)\n",
        "    else:\n",
        "        print(\"Found FairFace image zip; skipping re-download.\")\n",
        "\n",
        "    if not lbl_train.exists():\n",
        "        print(\"Downloading FairFace label train CSV…\")\n",
        "        gdown.download(id=FAIRFACE_LABEL_TRAIN_ID, output=str(lbl_train), quiet=False)\n",
        "    else:\n",
        "        print(\"Found FairFace label train CSV; skipping re-download.\")\n",
        "\n",
        "    if not lbl_val.exists():\n",
        "        print(\"Downloading FairFace label val CSV…\")\n",
        "        gdown.download(id=FAIRFACE_LABEL_VAL_ID, output=str(lbl_val), quiet=False)\n",
        "    else:\n",
        "        print(\"Found FairFace label val CSV; skipping re-download.\")\n",
        "\n",
        "    # Extract if images not yet extracted\n",
        "    if not is_fairface_present(FAIRFACE_DIR):\n",
        "        print(\"Extracting FairFace archives…\")\n",
        "        try:\n",
        "            extract_archive(ff_zip, FAIRFACE_DIR)\n",
        "        except Exception as e:\n",
        "            print(f\"Extraction issue for FairFace: {e}\")\n",
        "        # After extraction, we consider it “present” if images+labels are available\n",
        "        if is_fairface_present(FAIRFACE_DIR):\n",
        "            print(\"FairFace ready.\")\n",
        "        else:\n",
        "            print(\"FairFace may still be incomplete; verify files under:\", FAIRFACE_DIR)\n",
        "\n",
        "def ensure_kaggle_downloaded(dataset_slug: str, dest: Path, dataset_name_for_print: str):\n",
        "    \"\"\"\n",
        "    Ensure Kaggle dataset is present with extracted images.\n",
        "    Skips re-download if images already exist; otherwise downloads and extracts.\n",
        "    \"\"\"\n",
        "    dest.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if _any_images_in_dir(dest):\n",
        "        print(f\"Found existing {dataset_name_for_print} images in {dest}; skipping download.\")\n",
        "        return\n",
        "\n",
        "    # If archives are present (e.g., from a previous partial download), try extracting them first\n",
        "    if _any_archives_in_dir(dest):\n",
        "        print(f\"Found archives for {dataset_name_for_print} in {dest}; extracting…\")\n",
        "        extract_all_archives(dest)\n",
        "        if _any_images_in_dir(dest):\n",
        "            print(f\"{dataset_name_for_print} ready after extraction.\")\n",
        "            return\n",
        "\n",
        "    if RUNNING_IN_COLAB:\n",
        "        print(\"Running in Google Colab\")\n",
        "        # Check the environment variables exist\n",
        "        kaggle_username = userdata.get('KAGGLE_USERNAME')\n",
        "        kaggle_key = userdata.get('KAGGLE_KEY')\n",
        "        if kaggle_username and kaggle_key:\n",
        "            print(\"KAGGLE_USERNAME and KAGGLE_KEY environment variables are defined.\")\n",
        "        else:\n",
        "            print(\"KAGGLE_USERNAME or KAGGLE_KEY environment variables are NOT defined.\")\n",
        "            if not kaggle_username:\n",
        "                print(\"  - KAGGLE_USERNAME is not defined.\")\n",
        "            if not kaggle_key:\n",
        "                print(\"  - KAGGLE_KEY is not defined.\")\n",
        "            return\n",
        "    else:\n",
        "        print(\"Not running in Google Colab\")\n",
        "        if not kaggle_creds.exists():\n",
        "            print(f\"Kaggle credentials missing; cannot download {dataset_name_for_print}.\")\n",
        "            return\n",
        "\n",
        "    print(f\"Downloading {dataset_name_for_print} via Kaggle: {dataset_slug}\")\n",
        "    cmd = f'kaggle datasets download -d {dataset_slug} -p \"{dest}\"'\n",
        "    r = os.system(cmd)\n",
        "    if r != 0:\n",
        "        print(f\"Kaggle download failed for {dataset_slug}.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Extracting {dataset_name_for_print} archives…\")\n",
        "    extract_all_archives(dest)\n",
        "\n",
        "    if _any_images_in_dir(dest):\n",
        "        print(f\"{dataset_name_for_print} ready.\")\n",
        "    else:\n",
        "        print(f\"No images detected for {dataset_name_for_print} after extraction. Check contents in {dest}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9e98cb8-88be-420c-9bfd-9c5e44d21bb4",
      "metadata": {
        "id": "d9e98cb8-88be-420c-9bfd-9c5e44d21bb4"
      },
      "source": [
        "### Other helper utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e959c8e4-6b5f-41d3-84f7-95fa1c478e89",
      "metadata": {
        "id": "e959c8e4-6b5f-41d3-84f7-95fa1c478e89"
      },
      "outputs": [],
      "source": [
        "def extract_archive(archive_path: Path, dest_dir: Path):\n",
        "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "    ap = str(archive_path).lower()\n",
        "    print(f\"Extracting {archive_path} to {dest_dir}…\")\n",
        "    if ap.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(archive_path, 'r') as zf: zf.extractall(dest_dir)\n",
        "    elif ap.endswith((\".tar\", \".tar.gz\", \".tgz\", \".tar.bz2\", \".tbz\")):\n",
        "        import tarfile\n",
        "        with tarfile.open(archive_path, 'r:*') as tf: tf.extractall(dest_dir)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown archive format: {archive_path}\")\n",
        "    print(\"Done extracting files.\")\n",
        "\n",
        "def find_images(root: Path, include_dirs=None):\n",
        "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "    roots = [root]\n",
        "    if include_dirs:\n",
        "        roots = [root / d for d in include_dirs if (root / d).exists()] or [root]\n",
        "    files = []\n",
        "    for r in roots:\n",
        "        for p in r.rglob(\"*\"):\n",
        "            if p.suffix.lower() in exts and p.is_file(): files.append(p)\n",
        "    return files\n",
        "\n",
        "def to_rgb(img: Image.Image) -> Image.Image:\n",
        "    if img.mode in (\"RGBA\", \"LA\"):\n",
        "        bg = Image.new(\"RGB\", img.size, (255, 255, 255)); bg.paste(img, mask=img.split()[-1]); return bg\n",
        "    return img.convert(\"RGB\") if img.mode != \"RGB\" else img\n",
        "\n",
        "def center_crop_resize(img: Image.Image, size=224) -> Image.Image:\n",
        "    return ImageOps.fit(to_rgb(img), (size, size), method=Image.BILINEAR, bleed=0.0, centering=(0.5, 0.5))\n",
        "\n",
        "def safe_save_jpeg(img: Image.Image, path: Path):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    img.save(path, format=\"JPEG\", quality=JPEG_QUALITY, optimize=True)\n",
        "\n",
        "def phash_file(path: Path):\n",
        "    try:\n",
        "        with Image.open(path) as im: return imagehash.phash(to_rgb(im))\n",
        "    except Exception: return None\n",
        "\n",
        "def deduplicate_paths(paths, verbose_name=\"items\"):\n",
        "    seen, unique, dupes = {}, [], 0\n",
        "    for p in tqdm(paths, desc=f\"Dedup {verbose_name}\", leave=False):\n",
        "        h = phash_file(p)\n",
        "        if h is None: continue\n",
        "        if h in seen: dupes += 1; continue\n",
        "        seen[h] = p; unique.append(p)\n",
        "    if dupes: print(f\"• Removed {dupes} duplicates (pHash) from {verbose_name}.\")\n",
        "    return unique\n",
        "\n",
        "def check_image_rules(path: Path):\n",
        "    \"\"\"Return (ok: bool, reason: str). Applies only if APPLY_IMAGE_FILTERS=True.\"\"\"\n",
        "    try:\n",
        "        with Image.open(path) as im:\n",
        "            w, h = im.size\n",
        "            short_side = min(w, h)\n",
        "            ar = (w / h) if h else 0\n",
        "    except Exception as e:\n",
        "        return False, f\"unreadable ({e.__class__.__name__})\"\n",
        "\n",
        "    if not APPLY_IMAGE_FILTERS:\n",
        "        return True, \"\"\n",
        "\n",
        "    if short_side < MIN_SHORT_SIDE_PX:\n",
        "        return False, f\"min-dim too small (min={short_side}px < {MIN_SHORT_SIDE_PX}px)\"\n",
        "    if ar < MIN_ASPECT_RATIO or ar > MAX_ASPECT_RATIO:\n",
        "        return False, f\"aspect ratio {ar:.2f} outside [{MIN_ASPECT_RATIO:.2f},{MAX_ASPECT_RATIO:.2f}]\"\n",
        "    return True, \"\"\n",
        "\n",
        "def filter_paths_with_logging(paths, class_name: str):\n",
        "    kept = []\n",
        "    for p in tqdm(paths, desc=f\"Filter images ({class_name})\", leave=False):\n",
        "        ok, reason = check_image_rules(p)\n",
        "        if ok:\n",
        "            kept.append(p)\n",
        "        else:\n",
        "            if PRINT_REJECTION_MESSAGES:\n",
        "                print(f\"Reject [{class_name}]: {p} → {reason}\")\n",
        "    return kept\n",
        "\n",
        "def sample_n_stratified(df, group_cols, n_total, seed=SEED):\n",
        "    groups = df.groupby(group_cols)\n",
        "    combos = list(groups.groups.keys())\n",
        "    k = len(combos)\n",
        "    base_n = n_total // k\n",
        "    remainder = n_total - base_n * k\n",
        "\n",
        "    sampled = []\n",
        "    capacities = []\n",
        "    for key, g in groups:\n",
        "        take = min(base_n, len(g))\n",
        "        samp = g.sample(n=take, random_state=seed) if take > 0 else g.iloc[[]]\n",
        "        sampled.append(samp)\n",
        "        capacities.append((key, len(g) - take))\n",
        "    result = pd.concat(sampled, ignore_index=True)\n",
        "\n",
        "    if remainder > 0:\n",
        "        capacities.sort(key=lambda x: x[1], reverse=True)\n",
        "        i = 0\n",
        "        while remainder > 0 and i < len(capacities):\n",
        "            key, cap = capacities[i]\n",
        "            if cap > 0:\n",
        "                g = groups.get_group(key).drop(result.index, errors=\"ignore\")\n",
        "                take = min(1, len(g))\n",
        "                if take > 0:\n",
        "                    extra = g.sample(n=take, random_state=seed + remainder + i)\n",
        "                    result = pd.concat([result, extra], ignore_index=True)\n",
        "                    remainder -= take\n",
        "            i += 1\n",
        "    if remainder > 0:\n",
        "        rest = df.drop(result.index, errors=\"ignore\")\n",
        "        extra = rest.sample(n=min(remainder, len(rest)), random_state=seed + 999)\n",
        "        result = pd.concat([result, extra], ignore_index=True)\n",
        "\n",
        "    return result.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "def split_train_val_test(paths, split=SPLIT, seed=SEED):\n",
        "    paths = list(paths); random.Random(seed).shuffle(paths)\n",
        "    n = len(paths)\n",
        "    n_train = int(n * split[\"train\"])\n",
        "    n_val   = int(n * split[\"val\"])\n",
        "    return {\"train\": paths[:n_train],\n",
        "            \"val\":   paths[n_train:n_train+n_val],\n",
        "            \"test\":  paths[n_train+n_val:]}\n",
        "\n",
        "def copy_and_process(paths_by_split, class_name):\n",
        "    for split_name, paths in paths_by_split.items():\n",
        "        for src in tqdm(paths, desc=f\"Write {class_name}/{split_name}\", leave=False):\n",
        "            try:\n",
        "                with Image.open(src) as im:\n",
        "                    # Safety: re-check rules in case something slipped through\n",
        "                    ok, reason = check_image_rules(src)\n",
        "                    if not ok:\n",
        "                        if PRINT_REJECTION_MESSAGES:\n",
        "                            print(f\"Reject at write [{class_name}]: {src} → {reason}\")\n",
        "                        continue\n",
        "                    out = center_crop_resize(im, IMG_SIZE)\n",
        "                h = hashlib.md5(str(src).encode()).hexdigest()[:16]\n",
        "                out_path = OUT_DIR / split_name / class_name / f\"{h}.jpg\"\n",
        "                safe_save_jpeg(out, out_path)\n",
        "            except Exception as e:\n",
        "                if PRINT_REJECTION_MESSAGES:\n",
        "                    print(f\"Reject at write [{class_name}]: {src} → unreadable ({e.__class__.__name__})\")\n",
        "                continue"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fd11dc6-301f-4d7d-8408-25d486281830",
      "metadata": {
        "id": "7fd11dc6-301f-4d7d-8408-25d486281830"
      },
      "source": [
        "## 1) Download FairFace (padding=1.25) + labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "286be794-661e-4d09-8d71-9fa3964884a3",
      "metadata": {
        "id": "286be794-661e-4d09-8d71-9fa3964884a3"
      },
      "outputs": [],
      "source": [
        "print(\"== Ensuring FairFace availability ==\")\n",
        "ensure_fairface_downloaded()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85dbba98-79bf-40ff-a2d7-7739c57ab389",
      "metadata": {
        "id": "85dbba98-79bf-40ff-a2d7-7739c57ab389"
      },
      "source": [
        "## 2) Parse labels, adult-only filter, build strata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8d6db7-117b-4024-93a4-63e1adfadb65",
      "metadata": {
        "id": "7b8d6db7-117b-4024-93a4-63e1adfadb65"
      },
      "outputs": [],
      "source": [
        "def load_fairface_df(ff_dir: Path):\n",
        "    df_train = pd.read_csv(next(ff_dir.glob(\"*label_train*.csv\")))\n",
        "    df_val   = pd.read_csv(next(ff_dir.glob(\"*label_val*.csv\")))\n",
        "    df = pd.concat([df_train, df_val], ignore_index=True)\n",
        "\n",
        "    cols = {c.lower().strip(): c for c in df.columns}\n",
        "    def map_col(*cands):\n",
        "        for c in cands:\n",
        "            lc = c.lower().strip()\n",
        "            if lc in cols: return cols[lc]\n",
        "        for k in cols:\n",
        "            if any(cc in k for cc in cands): return cols[k]\n",
        "        return None\n",
        "    file_col   = map_col(\"file\", \"image\", \"path\")\n",
        "    age_col    = map_col(\"age\")\n",
        "    gender_col = map_col(\"gender\", \"sex\")\n",
        "    race_col   = map_col(\"race\", \"ethnicity\", \"race_7\", \"race_8\", \"race_4\")\n",
        "\n",
        "    if any(x is None for x in [file_col, age_col, gender_col, race_col]):\n",
        "        raise ValueError(f\"Could not map needed columns. Found {list(df.columns)}\")\n",
        "\n",
        "    df = df[[file_col, age_col, gender_col, race_col]].rename(\n",
        "        columns={file_col:\"file\", age_col:\"age\", gender_col:\"gender\", race_col:\"race\"}\n",
        "    )\n",
        "\n",
        "    # Resolve image paths inside extracted pad=1.25 pack (usually under train/ and val/)\n",
        "    img_root = ff_dir\n",
        "    def resolve_path(rel):\n",
        "        rel = str(rel).lstrip(\"./\\\\\")\n",
        "        cand = img_root / rel\n",
        "        if cand.exists(): return cand\n",
        "        for sub in [\"train\", \"val\", \"fairface-img-margin125-trainval/train\", \"fairface-img-margin125-trainval/val\"]:\n",
        "            c = img_root / sub / rel\n",
        "            if c.exists(): return c\n",
        "        hits = list(img_root.rglob(Path(rel).name))\n",
        "        return hits[0] if hits else None\n",
        "\n",
        "    df[\"path\"] = df[\"file\"].apply(resolve_path)\n",
        "    df = df[df[\"path\"].notna()].reset_index(drop=True)\n",
        "\n",
        "    # Adult-only: drop under-18 bins (exclude '0-2','3-9','10-19', any 'under <18' style)\n",
        "    def is_adult(age_lab: str):\n",
        "        s = str(age_lab).lower()\n",
        "        if \"10-19\" in s or \"0-2\" in s or \"3-9\" in s: return False\n",
        "        if \"under\" in s or \"<\" in s: return False\n",
        "        return True\n",
        "\n",
        "    for col in [\"age\", \"gender\", \"race\"]:\n",
        "        df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "    df = df[df[\"age\"].apply(is_adult)].reset_index(drop=True)\n",
        "    # (race × gender × adult-age-bin) strata to preserve FairFace balance in adult subset\n",
        "    df[\"strata\"] = df[\"race\"] + \" | \" + df[\"gender\"] + \" | \" + df[\"age\"]\n",
        "    return df\n",
        "\n",
        "fairface_df = load_fairface_df(FAIRFACE_DIR)\n",
        "print(\"FairFace (adult) candidates (pre-filter):\", len(fairface_df), \"  strata:\", len(fairface_df[\"strata\"].unique()))\n",
        "\n",
        "# Apply image-quality filters to FairFace before stratified sampling (maintains balance within the *filtered* adult pool)\n",
        "ff_keep_mask = []\n",
        "for row in tqdm(fairface_df.itertuples(index=False), total=len(fairface_df), desc=\"Filter images (human/FairFace)\", leave=False):\n",
        "    ok, reason = check_image_rules(row.path)\n",
        "    ff_keep_mask.append(ok)\n",
        "    if not ok and PRINT_REJECTION_MESSAGES:\n",
        "        print(f\"Reject [human]: {row.path} → {reason}\")\n",
        "fairface_df = fairface_df[np.array(ff_keep_mask, dtype=bool)].reset_index(drop=True)\n",
        "print(\"FairFace (adult) candidates (post-filter):\", len(fairface_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36119d3a-98ee-46fc-abba-6fb04459ba72",
      "metadata": {
        "id": "36119d3a-98ee-46fc-abba-6fb04459ba72"
      },
      "source": [
        "## 3) Stratified sample N=TARGET_PER_CLASS humans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44e3d9c5-900c-40b3-92ce-e6b327889c06",
      "metadata": {
        "id": "44e3d9c5-900c-40b3-92ce-e6b327889c06"
      },
      "outputs": [],
      "source": [
        "if len(fairface_df) < MIN_REQUIRED_PER_CLASS:\n",
        "    raise RuntimeError(f\"Not enough adult FairFace images ({len(fairface_df)}) to sample {TARGET_PER_CLASS}.\")\n",
        "human_sample = sample_n_stratified(fairface_df, [\"strata\"], TARGET_PER_CLASS, seed=SEED)\n",
        "human_paths = deduplicate_paths(human_sample[\"path\"].tolist(), \"human\")\n",
        "if len(human_paths) < MIN_REQUIRED_PER_CLASS:\n",
        "    print(\"Top-up from remaining adult pool (post-dedup).\")\n",
        "    chosen = set(map(str, human_paths))\n",
        "    remaining = [p for p in fairface_df[\"path\"].tolist() if str(p) not in chosen]\n",
        "    remaining = deduplicate_paths(remaining, \"human-topup-candidates\")\n",
        "    need = MIN_REQUIRED_PER_CLASS - len(human_paths)\n",
        "    human_paths += remaining[:need]\n",
        "    human_paths = human_paths[:MIN_REQUIRED_PER_CLASS]\n",
        "print(\"Human final:\", len(human_paths))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "830b591d-61eb-4e55-913e-62fc63117f84",
      "metadata": {
        "id": "830b591d-61eb-4e55-913e-62fc63117f84"
      },
      "source": [
        "## 4) Kaggle downloads: Avatars + Animal faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf0f8d3-ddca-4f93-9cf9-d984125d0da5",
      "metadata": {
        "id": "cbf0f8d3-ddca-4f93-9cf9-d984125d0da5"
      },
      "outputs": [],
      "source": [
        "AVATAR_DIR = RAW_DIR / \"avatars\"\n",
        "ANIMAL_DIR = RAW_DIR / \"animal_faces\"\n",
        "print(\"\\n== Ensuring Kaggle datasets availability ==\")\n",
        "ensure_kaggle_downloaded(KAGGLE_AVATARS_DATASET, RAW_DIR / \"avatars\", \"Avatars (Google Cartoon Set)\")\n",
        "ensure_kaggle_downloaded(KAGGLE_ANIMALFACES_DATASET, RAW_DIR / \"animal_faces\", \"Animal Faces (Dogs vs Cats)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6da3e936-de00-43e7-adb2-d853f05e44f1",
      "metadata": {
        "id": "6da3e936-de00-43e7-adb2-d853f05e44f1"
      },
      "source": [
        "## 5) Collect & sample Avatars (with filters + dedup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75c5a660-d368-4d11-9572-7325a41881dd",
      "metadata": {
        "id": "75c5a660-d368-4d11-9572-7325a41881dd"
      },
      "outputs": [],
      "source": [
        "avatar_all = find_images(AVATAR_DIR, include_dirs=AVATAR_INCLUDE_DIRS)\n",
        "avatar_all = filter_paths_with_logging(avatar_all, \"avatar\")\n",
        "avatar_all = deduplicate_paths(avatar_all, \"avatar\")\n",
        "if len(avatar_all) < MIN_REQUIRED_PER_CLASS:\n",
        "    raise RuntimeError(f\"Not enough avatar images ({len(avatar_all)}) for {TARGET_PER_CLASS}.\")\n",
        "avatar_paths = random.sample(avatar_all, TARGET_PER_CLASS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a05b8fa-2ed1-41d6-911d-d9c20e7f8206",
      "metadata": {
        "id": "1a05b8fa-2ed1-41d6-911d-d9c20e7f8206"
      },
      "source": [
        "## 6) Collect & sample Animal faces (with filters + dedup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d940620-a786-4678-b563-f9a1d2c1ffaf",
      "metadata": {
        "id": "2d940620-a786-4678-b563-f9a1d2c1ffaf"
      },
      "outputs": [],
      "source": [
        "animal_all = find_images(ANIMAL_DIR, include_dirs=[\"test\", \"train\"])\n",
        "animal_all = filter_paths_with_logging(animal_all, \"animal\")\n",
        "animal_all = deduplicate_paths(animal_all, \"animal\")\n",
        "if len(animal_all) < MIN_REQUIRED_PER_CLASS:\n",
        "    raise RuntimeError(f\"Not enough animal images ({len(animal_all)}) for {TARGET_PER_CLASS}.\")\n",
        "animal_paths = random.sample(animal_all, TARGET_PER_CLASS)\n",
        "print(len(animal_paths))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b0fe07-37cc-4f17-b3a5-4262b1524801",
      "metadata": {
        "id": "01b0fe07-37cc-4f17-b3a5-4262b1524801"
      },
      "source": [
        "## 7) Split & write ImageFolder with resize/crop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e6dbd4-129e-4367-9e19-ceb6daf70d78",
      "metadata": {
        "id": "13e6dbd4-129e-4367-9e19-ceb6daf70d78"
      },
      "outputs": [],
      "source": [
        "if OUT_DIR.exists():\n",
        "    print(\"Clearing old output:\", OUT_DIR); shutil.rmtree(OUT_DIR)\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "spl_human  = split_train_val_test(human_paths,  SPLIT, SEED)\n",
        "spl_avatar = split_train_val_test(avatar_paths, SPLIT, SEED)\n",
        "spl_animal = split_train_val_test(animal_paths, SPLIT, SEED)\n",
        "\n",
        "copy_and_process(spl_human,  \"human\")\n",
        "copy_and_process(spl_avatar, \"avatar\")\n",
        "copy_and_process(spl_animal, \"animal\")\n",
        "\n",
        "def count_images(root): return sum(1 for _ in root.rglob(\"*.jpg\"))\n",
        "for split_name in [\"train\", \"val\", \"test\"]:\n",
        "    for cls in [\"human\", \"avatar\", \"animal\"]:\n",
        "        print(f\"{split_name:5s}/{cls:7s}: {count_images(OUT_DIR / split_name / cls)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61139eb9-206d-4784-a725-c2bd0863e893",
      "metadata": {
        "id": "61139eb9-206d-4784-a725-c2bd0863e893"
      },
      "source": [
        "## 8) Manifest for traceability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff6ba9c-6d63-4b9a-8944-e5874b7dd4b8",
      "metadata": {
        "id": "cff6ba9c-6d63-4b9a-8944-e5874b7dd4b8"
      },
      "outputs": [],
      "source": [
        "manifest = {\n",
        "    \"seed\": SEED,\n",
        "    \"img_size\": IMG_SIZE,\n",
        "    \"classes\": {\"human\": len(human_paths), \"avatar\": len(avatar_paths), \"animal\": len(animal_paths)},\n",
        "    \"split\": SPLIT,\n",
        "    \"sources\": {\n",
        "        \"fairface_padding\": \"1.25\",\n",
        "        \"fairface_readme\": \"https://github.com/joojs/fairface\",\n",
        "        \"fairface_img_id\": FAIRFACE_IMG_PAD125_ID,\n",
        "        \"fairface_label_train_id\": FAIRFACE_LABEL_TRAIN_ID,\n",
        "        \"fairface_label_val_id\": FAIRFACE_LABEL_VAL_ID,\n",
        "        \"kaggle_avatars\": KAGGLE_AVATARS_DATASET,\n",
        "        \"kaggle_animals\":  KAGGLE_ANIMALFACES_DATASET,\n",
        "        \"avatar_include_dirs\": AVATAR_INCLUDE_DIRS,\n",
        "        \"animal_include_dirs\": ANIMAL_INCLUDE_DIRS,\n",
        "    },\n",
        "    \"target_per_class\": TARGET_PER_CLASS,\n",
        "    \"filters\": {\n",
        "        \"apply\": APPLY_IMAGE_FILTERS,\n",
        "        \"min_short_side_px\": MIN_SHORT_SIDE_PX,\n",
        "        \"min_ar\": MIN_ASPECT_RATIO,\n",
        "        \"max_ar\": MAX_ASPECT_RATIO,\n",
        "        \"print_rejections\": PRINT_REJECTION_MESSAGES,\n",
        "    },\n",
        "}\n",
        "(OUT_DIR / \"MANIFEST.json\").write_text(json.dumps(manifest, indent=2))\n",
        "print(\"\\nDataset build complete.\")\n",
        "print(\"Output (ImageFolder):\", OUT_DIR.resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J5HXwJI2NHOR",
      "metadata": {
        "id": "J5HXwJI2NHOR"
      },
      "outputs": [],
      "source": [
        "end_time = time.time()\n",
        "print(f\"Run started at: {time.ctime(start_time)}\")\n",
        "print(f\"Run ended at: {time.ctime(end_time)}\")\n",
        "elapsed_time_seconds = end_time - start_time\n",
        "elapsed_time_minutes = elapsed_time_seconds / 60\n",
        "print(f\"Total elapsed time: {elapsed_time_minutes:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cTtWN70CMAQP",
      "metadata": {
        "id": "cTtWN70CMAQP"
      },
      "source": [
        "# Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EcRcDa5oL24K",
      "metadata": {
        "id": "EcRcDa5oL24K"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#   drive.mount('/content/drive')\n",
        "# except Exception as e:\n",
        "#   print(f\"Error mounting Google Drive: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_data_to_drive():\n",
        "    \"\"\"Copies the data directory from Colab to Google Drive.\"\"\"\n",
        "    if not COLAB_DATA_DIR.exists():\n",
        "        print(f\"Source directory not found: {COLAB_DATA_DIR}\")\n",
        "        return\n",
        "\n",
        "    DRIVE_DATA_PARENT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Remove existing directory in Drive to avoid errors during copy\n",
        "    if DRIVE_DATA_DIR.exists():\n",
        "        print(f\"Removing existing data directory in Drive: {DRIVE_DATA_DIR}\")\n",
        "        shutil.rmtree(DRIVE_DATA_DIR)\n",
        "\n",
        "    print(f\"Copying data from {COLAB_DATA_DIR} to {DRIVE_DATA_DIR}...\")\n",
        "    try:\n",
        "        shutil.copytree(COLAB_DATA_DIR, DRIVE_DATA_DIR)\n",
        "        print(\"Data successfully copied to Google Drive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error copying data to Drive: {e}\")\n",
        "\n",
        "def copy_data_from_drive():\n",
        "    \"\"\"Copies the data directory from Google Drive to Colab.\"\"\"\n",
        "    if not DRIVE_DATA_DIR.exists():\n",
        "        print(f\"Source directory not found in Drive: {DRIVE_DATA_DIR}\")\n",
        "        print(\"Please ensure Google Drive is mounted and the data exists at the specified path.\")\n",
        "        return\n",
        "\n",
        "    # Remove existing directory in Colab to avoid errors during copy\n",
        "    if COLAB_DATA_DIR.exists():\n",
        "        print(f\"Removing existing data directory in Colab: {COLAB_DATA_DIR}\")\n",
        "        shutil.rmtree(COLAB_DATA_DIR)\n",
        "\n",
        "    print(f\"Copying data from {DRIVE_DATA_DIR} to {COLAB_DATA_DIR}...\")\n",
        "    try:\n",
        "        shutil.copytree(DRIVE_DATA_DIR, COLAB_DATA_DIR)\n",
        "        print(\"Data successfully copied to Colab environment.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error copying data from Drive: {e}\")"
      ],
      "metadata": {
        "id": "0p9HF6hVDQ3s"
      },
      "id": "0p9HF6hVDQ3s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy_data_to_drive()\n",
        "# copy_data_from_drive()"
      ],
      "metadata": {
        "id": "1WcxrKLXej7e"
      },
      "id": "1WcxrKLXej7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def zip_data_directory():\n",
        "    \"\"\"Zips the 'data' directory.\"\"\"\n",
        "    data_dir = Path(\"data\")\n",
        "    if not data_dir.exists():\n",
        "        print(f\"Error: Directory '{data_dir}' not found.\")\n",
        "        return\n",
        "    zip_filename = data_dir.with_suffix(\".zip\")\n",
        "    print(f\"Creating zip archive: {zip_filename} from {data_dir}\")\n",
        "    try:\n",
        "        shutil.make_archive(str(data_dir), 'zip', data_dir)\n",
        "        print(\"Zip archive created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error zipping directory: {e}\")\n",
        "\n",
        "def unzip_data_directory():\n",
        "    \"\"\"Unzips the 'data.zip' archive.\"\"\"\n",
        "    zip_filename = Path(\"data.zip\")\n",
        "    if not zip_filename.exists():\n",
        "        print(f\"Error: Zip file '{zip_filename}' not found.\")\n",
        "        return\n",
        "    extract_dir = Path(\"data\")\n",
        "    if extract_dir.exists():\n",
        "        print(f\"Warning: Directory '{extract_dir}' already exists. Contents may be overwritten.\")\n",
        "    print(f\"Extracting zip archive: {zip_filename} to {extract_dir}\")\n",
        "    try:\n",
        "        shutil.unpack_archive(str(zip_filename), str(extract_dir), 'zip')\n",
        "        print(\"Zip archive extracted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error unzipping archive: {e}\")"
      ],
      "metadata": {
        "id": "s_xEXzBXDegp"
      },
      "id": "s_xEXzBXDegp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iXilnPfc279j",
      "metadata": {
        "id": "iXilnPfc279j"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "# from pathlib import Path\n",
        "\n",
        "# ZIP_NAME = \"data.zip\"\n",
        "\n",
        "# # Define the source and destination paths\n",
        "# source_zip = DRIVE_DATA_DIR / ZIP_NAME\n",
        "# # Ensure the parent directory in Drive exists\n",
        "# DRIVE_DATA_PARENT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "# # Define the destination path in Drive for the zip file\n",
        "# drive_zip_destination = Path(ZIP_NAME)\n",
        "\n",
        "# if not source_zip.exists():\n",
        "#     print(f\"Error: Source zip file '{source_zip}' not found.\")\n",
        "# else:\n",
        "#     print(f\"Copying '{source_zip}' to '{drive_zip_destination}'...\")\n",
        "#     try:\n",
        "#         # Use shutil.copy2 to preserve metadata (optional, but good practice)\n",
        "#         shutil.copy2(source_zip, drive_zip_destination)\n",
        "#         unzip_data_directory()\n",
        "#         print(\"Zip file successfully copied from Google Drive and unzipped.\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error copying zip file to Drive: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "091OewsV2w7r",
      "metadata": {
        "id": "091OewsV2w7r"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "# from pathlib import Path\n",
        "\n",
        "# # zip_data_directory()\n",
        "\n",
        "# # Define the source and destination paths\n",
        "# source_zip = Path(\"data.zip\")\n",
        "# # Ensure the parent directory in Drive exists\n",
        "# DRIVE_DATA_PARENT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "# # Define the destination path in Drive for the zip file\n",
        "# drive_zip_destination = DRIVE_DATA_DIR / source_zip.name\n",
        "\n",
        "# # Ensure the destination directory in Drive exists\n",
        "# drive_zip_destination.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# if not source_zip.exists():\n",
        "#     print(f\"Error: Source zip file '{source_zip}' not found.\")\n",
        "# else:\n",
        "#     print(f\"Copying '{source_zip}' to '{drive_zip_destination}'...\")\n",
        "#     try:\n",
        "#         # Use shutil.copy2 to preserve metadata (optional, but good practice)\n",
        "#         shutil.copy2(source_zip, drive_zip_destination)\n",
        "#         print(\"Zip file successfully copied to Google Drive.\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error copying zip file to Drive: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleanup Utilities"
      ],
      "metadata": {
        "id": "hejchZ5EC5VP"
      },
      "id": "hejchZ5EC5VP"
    },
    {
      "cell_type": "code",
      "source": [
        "def _is_subpath(child: Path, parent: Path) -> bool:\n",
        "    try:\n",
        "        child.resolve().relative_to(parent.resolve())\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _safe_rmtree(p: Path):\n",
        "    if p.exists():\n",
        "        if _is_subpath(p, WORK_DIR):\n",
        "            shutil.rmtree(p)\n",
        "            print(f\"Removed: {p}\")\n",
        "        else:\n",
        "            print(f\"Skip delete (outside WORK_DIR): {p}\")\n",
        "\n",
        "def cleanup_generated_dataset():\n",
        "    \"\"\"Remove only the generated ImageFolder dataset (data/final).\"\"\"\n",
        "    _safe_rmtree(OUT_DIR)\n",
        "\n",
        "def cleanup_all_datasets():\n",
        "    \"\"\"Remove initial raw downloads (data/raw) and the generated dataset (data/final).\"\"\"\n",
        "    _safe_rmtree(OUT_DIR)\n",
        "    _safe_rmtree(RAW_DIR)\n",
        "    _safe_rmtree(TMP_DIR)\n",
        "\n",
        "def cleanup_datasets(mode: str = \"generated\"):\n",
        "    \"\"\"\n",
        "    mode ∈ {\"generated\", \"all\"}\n",
        "      - \"generated\": remove only data/final\n",
        "      - \"all\":       remove data/raw, data/final, data/tmp\n",
        "    \"\"\"\n",
        "    mode = mode.lower().strip()\n",
        "    if mode == \"generated\":\n",
        "        cleanup_generated_dataset()\n",
        "    elif mode == \"all\":\n",
        "        cleanup_all_datasets()\n",
        "    else:\n",
        "        raise ValueError(\"mode must be 'generated' or 'all'\")"
      ],
      "metadata": {
        "id": "UoDeIBsdCxRM"
      },
      "id": "UoDeIBsdCxRM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B7fjU9QxJC4T",
      "metadata": {
        "id": "B7fjU9QxJC4T"
      },
      "outputs": [],
      "source": [
        "# cleanup_all_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffbad944",
      "metadata": {
        "id": "ffbad944"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# with open('/content/drive/My Drive/foo.txt', 'w') as f:\n",
        "#   f.write('Hello Google Drive!')\n",
        "# !cat /content/drive/My\\ Drive/baa.txt\n",
        "# drive.flush_and_unmount()\n",
        "# print('All changes made in this colab session should now be visible in Drive.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for Duplicates\n",
        "Every animal image appears twice. Used this code to verify that before changing the import code above."
      ],
      "metadata": {
        "id": "9QlfmAaPrD1_"
      },
      "id": "9QlfmAaPrD1_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897bf556"
      },
      "source": [
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "def find_all_image_filenames(root_dir):\n",
        "    \"\"\"Recursively finds all image filenames (basename) within a directory.\"\"\"\n",
        "    image_filenames = []\n",
        "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}\n",
        "    for root, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if os.path.splitext(file)[1].lower() in image_extensions:\n",
        "                image_filenames.append(file)\n",
        "    return image_filenames\n",
        "\n",
        "# Find all image filenames\n",
        "all_animal_image_filenames = find_all_image_filenames(ANIMAL_DIR)\n",
        "\n",
        "# Count the occurrences of each filename\n",
        "filename_counts = Counter(all_animal_image_filenames)\n",
        "\n",
        "# Find filenames that appear more than once\n",
        "duplicate_filenames = {filename: count for filename, count in filename_counts.items() if count > 1}\n",
        "\n",
        "print(f\"Total number of animal image files found: {len(all_animal_image_filenames)}\")\n",
        "if duplicate_filenames:\n",
        "    print(f\"Number of filenames appearing more than once: {len(duplicate_filenames)}\")\n",
        "else:\n",
        "    print(\"No duplicate filenames found in the animal_faces directory structure.\")"
      ],
      "id": "897bf556",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}